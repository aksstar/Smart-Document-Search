{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install libreoffice-writer"
      ],
      "metadata": {
        "id": "ur6PaKS3wxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb langchain langchain_community docx2txt langchain-core langchain-google-vertexai google-cloud-aiplatform langchain-experimental gradio unstructured PyPDF2 python-docx python-pptx"
      ],
      "metadata": {
        "id": "H1ThlWhiP_BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Define project information\n",
        "    PROJECT_ID = \"PROJECT_ID\"  # @param {type:\"string\"}\n",
        "    LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "    # Initialize Vertex AI\n",
        "    import vertexai\n",
        "\n",
        "    vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "h5C6mbCptbfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpsfr2nNLG9Y"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import VertexAIEmbeddings\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import os\n",
        "import pptx\n",
        "import gradio as gr\n",
        "from langchain.llms import VertexAI\n",
        "from langchain.retrievers.document_compressors import LLMChainFilter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from vertexai.language_models import TextGenerationModel\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import Docx2txtLoader\n",
        "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
        "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "vertex_embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")\n",
        "\n",
        "\n",
        "def pretty_print_docs(docs):\n",
        "    \"\"\"Displays loaded documents in a structured format\"\"\"\n",
        "    print(\n",
        "        f\"\\n{'-' * 100}\\n\".join(\n",
        "            [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
        "        )\n",
        "    )\n",
        "\n",
        "def generate_embeddings_and_vector(texts):\n",
        "    \"\"\"\n",
        "    Creates text embeddings using Vertex AI and builds a Chroma vector index\n",
        "\n",
        "    Args:\n",
        "        texts: A list of text chunks\n",
        "\n",
        "    Returns:\n",
        "        A Chroma vector index ready for similarity search\n",
        "    \"\"\"\n",
        "    vector_index = Chroma.from_texts(texts, vertex_embeddings).as_retriever()\n",
        "    return vector_index\n",
        "\n",
        "\n",
        "def get_similar_documents(vector_index, search_query):\n",
        "    \"\"\"\n",
        "    Finds documents semantically relevant to a query using the vector index\n",
        "\n",
        "    Args:\n",
        "        vector_index: The Chroma vector index to search within\n",
        "        search_query: The user's search query\n",
        "\n",
        "    Returns:\n",
        "        A list of relevant documents\n",
        "    \"\"\"\n",
        "    docs = vector_index.get_relevant_documents(search_query)\n",
        "    return docs\n",
        "\n",
        "\n",
        "def generate_final_response(docs, search_query):\n",
        "    \"\"\"\n",
        "    Generates a concise and informative answer to the user's query, leveraging the provided documents for context.\n",
        "\n",
        "    Args:\n",
        "        docs: A list of relevant documents (likely as LangChain Document objects).\n",
        "        search_query: The user's search query.\n",
        "\n",
        "    Returns:\n",
        "        A text string containing the generated answer.\n",
        "    \"\"\"\n",
        "\n",
        "    parameters = {\n",
        "        \"candidate_count\": 1,\n",
        "        \"max_output_tokens\": 1024,\n",
        "        \"temperature\": 0.9,\n",
        "        \"top_p\": 1\n",
        "    }\n",
        "\n",
        "    prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    Helpful Answer: \"\"\".format(context=docs, question=search_query)\n",
        "\n",
        "    model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
        "    response = model.predict(prompt_template, **parameters)\n",
        "\n",
        "    print(response.text)\n",
        "\n",
        "    return response.text\n",
        "\n",
        "\n",
        "def process_file(fileobj, search_query):\n",
        "    \"\"\"\n",
        "    Loads a supported document, extracts its text content, and generates an answer to a provided query based on the document.\n",
        "\n",
        "    Args:\n",
        "        fileobj: A file-like object representing the document.\n",
        "        search_query: The user's question about the document.\n",
        "\n",
        "    Returns:\n",
        "        A text string containing the answer, or \"Failed to load the document\" if an error occurs.\n",
        "    \"\"\"\n",
        "\n",
        "    file_path = fileobj.name\n",
        "    filename, file_extension = os.path.splitext(file_path)\n",
        "\n",
        "    if file_extension == '.txt':\n",
        "        # return do_something(file_path)\n",
        "        loader = TextLoader(file_path)\n",
        "        documents = loader.load()\n",
        "\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        context = \"\\n\\n\".join(str(p.page_content) for p in documents)\n",
        "        texts = text_splitter.split_text(context)\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        # return do_something(file_path)\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        documents = loader.load_and_split()\n",
        "\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        context = \"\\n\\n\".join(str(p.page_content) for p in documents)\n",
        "        texts = text_splitter.split_text(context)\n",
        "\n",
        "    if file_extension == '.pptx' or file_extension == '.ppt':\n",
        "        # return do_something(file_path)\n",
        "        loader = UnstructuredPowerPointLoader(file_path)\n",
        "        documents = loader.load_and_split()\n",
        "\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        context = \"\\n\\n\".join(str(p.page_content) for p in documents)\n",
        "        texts = text_splitter.split_text(context)\n",
        "\n",
        "    if file_extension == '.docx' or file_extension == '.doc':\n",
        "        # return do_something(file_path)\n",
        "        loader = UnstructuredWordDocumentLoader(file_path)\n",
        "        documents = loader.load_and_split()\n",
        "\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        context = \"\\n\\n\".join(str(p.page_content) for p in documents)\n",
        "        texts = text_splitter.split_text(context)\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        # return do_something(file_path)\n",
        "        loader = CSVLoader(file_path)\n",
        "        documents = loader.load()\n",
        "\n",
        "        # text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        texts = [str(p.page_content) for p in documents]\n",
        "        # texts = text_splitter.split_text(context)\n",
        "\n",
        "    if len(texts) > 0:\n",
        "\n",
        "        vector_index = generate_embeddings_and_vector(texts)\n",
        "\n",
        "        llm = VertexAI(model_name=\"gemini-pro\")\n",
        "        _filter = LLMChainFilter.from_llm(llm)\n",
        "        compression_retriever = ContextualCompressionRetriever(\n",
        "            base_compressor=_filter, base_retriever=vector_index\n",
        "        )\n",
        "\n",
        "\n",
        "        compressed_docs = compression_retriever.get_relevant_documents(\n",
        "            search_query\n",
        "        )\n",
        "        context_text = [i.page_content for i in compressed_docs]\n",
        "        response_text = generate_final_response(context_text, search_query)\n",
        "        # print(compressed_docs)\n",
        "        pretty_print_docs(compressed_docs)\n",
        "        # return docs[0].page_content\n",
        "        return response_text\n",
        "\n",
        "    else:\n",
        "        return \"Failed to load the document\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio UI"
      ],
      "metadata": {
        "id": "gsThyeE_q-zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Text Embeddings + ChromaDB + Text Bison\"):\n",
        "\n",
        "            app = gr.Interface(\n",
        "                fn=process_file,\n",
        "                inputs=[\"file\", \"text\"],\n",
        "                outputs=[\"textbox\"],\n",
        "                title=\"Question Answering bot\",\n",
        "                description=\"Input context and question, then get answers!\",\n",
        "            )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "-VIFOwOrP7Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zEvpIEcivZ3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IscuD24jjkXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}